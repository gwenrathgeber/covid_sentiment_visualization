{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvAm7m1J4_HZ"
   },
   "source": [
    "# Problem Statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waW1jw1V74lN"
   },
   "source": [
    "With the recent series of unfortunate events that have happened, the COVID-19 pandemic being a major one; social media has been a constant stream of valuable information that could shed light on the state of the country. Twitter is a great text based social medium in which Natural Language Processing (NLP) techniques can be implemented on. Hence in this project, the goal is to conduct a sentiment analysis to determine the polarity of tweets related to COVID-19 and mapped them geographically across the United States. \n",
    "\n",
    "The nation's response to the pandemic has been largely regional and state-based in nature. Some states have enacted strictly enforced stay-at-home policies, while others have provided less stringent guidelines paired with instructions on a federal level. From this analysis, we would be able to observe the response of the nation according to respective state and federal guidelines. Comparing the sentiment analysis of tweets across the United States with respect to both the local policies on social distancing and the occurrences of the pandemic in those areas.\n",
    " \n",
    "\n",
    "In this project, we make initial steps toward designing and implementing a web-tool or an app for tracking and monitoring geo-tagged tweets during the pandemic, in close to real time.\n",
    "\n",
    "While traditional methods for alerting on such events rely on official information derived from official sources (e.g. CDC), our focus is attempting to utilize social media activity to identify these events and alert when an event first occurs. The question we look at primarily here is, given a sea of text content from social media platforms, how is the general public's response to pandemic guidelines and policies and how quickly does the public respond from the time a policy has been implemented? And what sort of implementation would be valuable?\n",
    "\n",
    "*Topic Extraction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzAZk7uZ4_EP"
   },
   "source": [
    "# Executive Summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iH_nHYuV8NzB"
   },
   "source": [
    "The workflow of the projects is as follows:\n",
    "\n",
    "- Importing packages, obtaining COVID-19 geo-tagged tweet ids \n",
    "- Hydrating tweets \n",
    "- Exploratory data analysis of tweets collected \n",
    "- Sentiment analysis of tweets\n",
    "- Evaluation\n",
    "- Visualizations \n",
    "- Conclusions and Recommendations\n",
    "\n",
    "For the purposes of this project, we pulled tweets from the Institute of Electrical and Electronics Engineers' (IEEE) Data Repository. The data mining process was an interesting but long process. First, obtaining a dataset of COVID-19 related tweet ids totalling to ~168,493 (at the time). Next, hydrating those tweet ids to obtain the Twitter information of those tweets. The tweets were then filtered for only those from the United States (primary focus) with state information. Our final tweets count was ~65,000. \n",
    "\n",
    "A sentiment analysis was then conducted using TextBlob determining the polarity of the tweets ranging from overwhelming positive on one end to overwhelming negative on the other. TextBlob is a popular Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation etc. \n",
    "\n",
    "The sentiment analysis data is then mapped using the geo-tagged information from these tweets to build a visual timeline of the country's overall sentiment on COVID-19. This opens up the ability for us to view the public's response on the events that unfold throughout the current pandemic. It also allows us to dive into deeper issues regarding response times and help us make better decisions in health policymaking especially in the event of the pandemic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cxucZ9t4_BB"
   },
   "source": [
    "# Contents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d45eHK_K8OmP"
   },
   "source": [
    "- [Data Dictionary](#Data-Dictionary)\n",
    "- [Package Import](#Package-Import)\n",
    "- [Scraping COVID-19 Geo Tagged Tweet URLs](#Scraping-COVID-19-Geo-Tagged-Tweet-URLs)\n",
    "- [Hydrating Tweets using TWARC API](#Hydrating-Tweets-using-TWARC-API)\n",
    "- [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA))\n",
    "- [Modeling](#Modeling)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "- [Reference](#Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVDzsSsj4-9O"
   },
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0CzJla59yCG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u20S3yVr4-0A"
   },
   "source": [
    "# Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTMT3Zkw7AB8"
   },
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.pipeline import Pipeline\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# Modeling Packages\n",
    "\n",
    "# Twitter\n",
    "# pip install textblob\n",
    "\n",
    "# Data Obtaining and Cleaning Packages\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rp5q6cmr6vAj"
   },
   "source": [
    "# Scraping COVID-19 Geo Tagged Tweet URLs \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sourced our tweets from [this collection](https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset). It turned out that, once logged in, the page contains a list of direct links to the `.csv` files of tweet IDs for each day, so we downloaded the specific `<div>` containing this list and used `BeautifulSoup` and `requests` to download the files. Finally, we concatenate the full list and converted the `.csv` to a `.txt` out of python for easy access from `twarc`.\n",
    "\n",
    "If you are following this process, ensure you have `pip install twarc`, and you can then run the shell command `twarc hydrate ids.txt > tweets.jsonl` in the folder with the `ids.txt` file to download all the tweets in json form. This process took about 20 minutes for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"../data/data_links.html\"), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csvs = []\n",
    "for i, link in list(enumerate(links)):\n",
    "    file = requests.get(link)\n",
    "    title = re.findall('(\\w+)(\\.\\w+)+(?!.*(\\w+)(\\.\\w+)+)', link)\n",
    "    title = ''.join(list(title[0]))\n",
    "    decoded_content = file.content.decode('utf-8')\n",
    "\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    my_list = list(cr)\n",
    "    all_csvs.append(pd.DataFrame(my_list))\n",
    "    pd.DataFrame(my_list).to_csv('../data/tweet_ids/' + title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = pd.concat(all_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids.to_csv('../data/tweet_ids/all_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUlniPTH5sSD"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDTlUI3GAPjK"
   },
   "source": [
    "## Unsupervised Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBdQHSOIoIU-"
   },
   "source": [
    "### Analyzing Twitter data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lqk7djVIluRD"
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://medium.com/shiyan-boxer/2020-us-presidential-election-twitter-sentiment-analysis-and-visualization-89e58a652af5\n",
    "# Big thanks to Shiyan Boxer\n",
    "\n",
    "\n",
    "class TweetAnalyzer():\n",
    "    \"\"\"\n",
    "    Functionality for analyzing and categorizing content from tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        return TextBlob(self.clean_tweet(tweet), analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet['full_text']\n",
    "                                for tweet in tweets], columns=['full_text'])\n",
    "        df['id'] = np.array([tweet['id'] for tweet in tweets])\n",
    "        df['date'] = np.array([tweet['created_at'] for tweet in tweets])\n",
    "        df['city'] = [tweet['place']['full_name'] for tweet in tweets]\n",
    "        df['country_code'] = [tweet['place']['country_code']\n",
    "                              for tweet in tweets]\n",
    "        df['country'] = [tweet['place']['country'] for tweet in tweets]\n",
    "        df['coordinates'] = [tweet['coordinates']['coordinates']\n",
    "                             for tweet in tweets]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .jsonl of all tweets\n",
    "\n",
    "all_tweets = []\n",
    "with open('../data/tweets.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        all_tweets.append(result)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets which do not have proper geo fields\n",
    "all_tweets = [tweet for tweet in all_tweets if type(tweet['place']) == dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TweetAnalyzer class to convert our .jsonl to a pandas dataframe\n",
    "ta = TweetAnalyzer()\n",
    "df = ta.tweets_to_data_frame(all_tweets)\n",
    "\n",
    "# Keep only US data\n",
    "df = df[df['country_code'] == 'US']\n",
    "\n",
    "# Keep only tweets with convenient state labels\n",
    "df['state'] = [city[-2:] for city in df['city']]\n",
    "\n",
    "valid_states = ['OH', 'CA', 'MA', 'FL', 'IL', 'MD', 'NC', 'NY', 'AZ',\n",
    "                'LA', 'TX', 'UT', 'GA', 'NV', 'MI', 'NJ', 'IN', 'ME', 'KS', 'VA',\n",
    "                'MN', 'TN', 'PA', 'SC', 'WI', 'NM', 'OR', 'MO', 'WA', 'DC',\n",
    "                'AL', 'CT', 'ID', 'KY', 'MS', 'CO', 'OK', 'HI', 'AR', 'VT', 'RI',\n",
    "                'NH', 'MT', 'DE', 'NE',  'SD', 'IA', 'ND', 'WV',  'AK',\n",
    "                'WY']\n",
    "\n",
    "df = df[df['state'].isin(valid_states)]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then needed to split our data into two halves to share the analysis time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:32316, :].to_csv(\n",
    "    '../data/tweets/cleaned_tweets_first_half.csv', index=False)\n",
    "df.loc[32317:, :].to_csv(\n",
    "    '../data/tweets/cleaned_tweets_second_half.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets/cleaned_tweets_second_half.csv')\n",
    "\n",
    "s = time.time()\n",
    "for i, tweet in enumerate(df['full_text']):\n",
    "    analysis = ta.analyze_sentiment(tweet).sentiment\n",
    "    df.loc[i, 'classification'] = analysis[0]\n",
    "    df.loc[i, 'p_pos'] = analysis[1]\n",
    "    df.loc[i, 'p_neg'] = analysis[2]\n",
    "    if i % 100 == 0:\n",
    "        print(\n",
    "            f'{i} of {df.shape[0]}, time elapsed: {(time.time() - s) / 60} minutes')\n",
    "        df.to_csv('../data/tweets/analyzed_tweets_second_half.csv', index=False)\n",
    "\n",
    "df.to_csv('../data/tweets/analyzed_tweets_second_half.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total runtime for one half of the data was `32200 of 32278, time elapsed: 2317.5798520445824 minutes\n",
    "`\n",
    "Total runtime for the other half was `32300 of 32317, time elapsed: 1938.4947881817818 minutes\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-merging files after we processed them on separate computers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('../data/tweets/analyzed_tweets_first_half.csv')\n",
    "df_2 = pd.read_csv('../data/tweets/analyzed_tweets_second_half.csv')\n",
    "df_3 = pd.concat([df_1, df_2])\n",
    "df_3.to_csv('../data/tweets/analyzed_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a slightly different version that uses `\\n` as seperator and `\\t` as text delimiter, which Tableau found easier to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv('../data/tweets/analyzed_tweets_for_tableau.csv',\n",
    "            index=False, sep='\\n', quotechar='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDyF7MzuoNHV"
   },
   "source": [
    "## Visualizing Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9wqhPbJm0bm"
   },
   "outputs": [],
   "source": [
    "# Get average length over all tweets:\n",
    "print(np.mean(df['len']))\n",
    "\n",
    "# Get the number of likes for the most liked tweet:\n",
    "print(np.max(df['likes']))\n",
    "\n",
    "# Get the number of retweets for the most retweeted tweet:\n",
    "print(np.max(df['retweets']))\n",
    "\n",
    "print(df.head(10))\n",
    "\n",
    "# Time Series\n",
    "# time_likes = pd.Series(data=df['len'].values, index=df['date'])\n",
    "# time_likes.plot(figsize=(16, 4), color='r')\n",
    "# plt.show()\n",
    "\n",
    "# time_favs = pd.Series(data=df['likes'].values, index=df['date'])\n",
    "# time_favs.plot(figsize=(16, 4), color='r')\n",
    "# plt.show()\n",
    "\n",
    "# time_retweets = pd.Series(data=df['retweets'].values, index=df['date'])\n",
    "# time_retweets.plot(figsize=(16, 4), color='r')\n",
    "# plt.show()\n",
    "\n",
    "# Layered Time Series:\n",
    "time_likes = pd.Series(data=df['likes'].values, index=df['date'])\n",
    "time_likes.plot(figsize=(16, 4), label=\"likes\", legend=True)\n",
    "\n",
    "time_retweets = pd.Series(data=df['retweets'].values, index=df['date'])\n",
    "time_retweets.plot(figsize=(16, 4), label=\"retweets\", legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4TYPsDATFn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VedZWcXL6GtP"
   },
   "source": [
    "# Conclusions and Recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we analyzed the sentiments of COVID-19-related tweets in several ways. The overall trend shows that the public has been more optimistic over time. Digging into the dual-dimensional sentiment analysis we conducted, we found that the sentiment “Positive” went down initially and towards the end, and “Negative” went up through the height of the pandemic. We were also able to see the respective sentiment consensus by state. Our results reflect the general reaction of the 'first wave' of the pandemic and the political climate during this time. \n",
    "\n",
    "The fight against COVID-19 not only needs the guidance from the government but also a positive attitude from the public. Our analysis provides a potential approach to reveal the public’s sentiment status and help institutions respond timely to it.\n",
    "\n",
    "Our analysis has shown some relationships between geographic data \n",
    "and the general sentiments of the state during the pandemic. Moving forward, introducing more granular data such confirmed cases' growth and adding additional dimensionality to our sentiment analysis would help provide a more comprehensive picture. Allowing us to generate more insights into hardest-hit areas, demographic of those affected; enabling institutions and government to take affirmative action based on this valuable information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zszvWu3d6Gmx"
   },
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHnKjLgR4MP2"
   },
   "source": [
    "- COVID-19 Geo Tagged Tweets Dataset: https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset\n",
    "- Package for Hydrating Tweets: https://github.com/DocNow/twarc\n",
    "- Unsupervised Sentiment Analysis (K Means Clustering): https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483\n",
    "- Recommended Python libraries for Sentiment Analysis: https://www.iflexion.com/blog/sentiment-analysis-python\n",
    "- Everything You Need to Know About Sentiment Analysis: https://monkeylearn.com/sentiment-analysis/\n",
    "- Twitter Sentiment Analysis with Python and NLTK: http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\n",
    "- Is it possible to do sentiment analysis of unlabelled text using word2vec model?: https://stackoverflow.com/questions/61185290/is-it-possible-to-do-sentiment-analysis-of-unlabelled-text-using-word2vec-model\n",
    "- Making a request to download csv: https://stackoverflow.com/questions/35371043/use-python-requests-to-download-csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSlNfIaqNSL7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
