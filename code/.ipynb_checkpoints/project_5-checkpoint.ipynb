{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![ga_logo](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png)Project 5: Sentiment Analysis of COVID-19-related Tweets  \n",
    "\n",
    "\n",
    "#### Eu Jin Lee [GitHub](https://github.com/missingNA) [LinkedIn](https://www.linkedin.com/in/eeujinlee/)  \n",
    "\n",
    "#### Gwen Rathgeber [GitHub](https://git.generalassemb.ly/gwenrathgeber) [LinkedIn](https://www.linkedin.com/in/gwenrathgeber/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvAm7m1J4_HZ"
   },
   "source": [
    "## Problem Statement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waW1jw1V74lN"
   },
   "source": [
    "Throughout the course of the COVID-19 pandemic, social media has been filled with a constant stream of valuable information that could shed light on the state of the world. Twitter is a great text-based social medium for applications of Natural Language Processing (NLP) techniques. In this project, our goal is to visually represent the mood of the United States over the course of the COVID-19 pandemic. \n",
    "\n",
    "The nation's response to the pandemic has been highly regional in nature. Some states had larger earlier or delayed spikes in case rates, for example. States varied on their timeline of enacting and enforcing stay-at-home policies. We hope to visualize the response of the nation along those significant state boundaries. \n",
    " \n",
    "We also make initial steps toward creating a web-based tool or app for tracking and monitoring the mood of the country in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uzAZk7uZ4_EP"
   },
   "source": [
    "## Executive Summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iH_nHYuV8NzB"
   },
   "source": [
    "The workflow of the projects is as follows:\n",
    "\n",
    "- Importing packages, obtaining COVID-19 geo-tagged tweet ids \n",
    "- Hydrating tweets \n",
    "- Sentiment analysis of tweets\n",
    "- Evaluation\n",
    "- Visualization Deliverable\n",
    "- Conclusions and Recommendations\n",
    "\n",
    "For the purposes of this project, we pulled tweet ids from the Institute of Electrical and Electronics Engineers' (IEEE) Data Repository. The data mining process was an interesting but long process. First, obtaining a dataset of COVID-19 related tweet ids totalling to ~168,493 (at the time). Next, we hydrated those tweet ids using the `twarc` Twitter API wrapper to obtain the full `.json` data of those tweets. The tweets were then filtered for only those from the United States (primary focus) with state information. Our final tweets count was ~65,000. \n",
    "\n",
    "A sentiment analysis was then conducted using TextBlob, determining the polarity of the tweets on a positive-to-negative spectrum. TextBlob is a popular Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation etc. \n",
    "\n",
    "The sentiment analysis data is then mapped in Tableau using the geo-tagged information from these tweets to build a visual timeline of the country's overall sentiment on COVID-19. This opens up the ability for us to view the public's response to events over the course of the pandemic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents \n",
    "\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Executive Summary](#Executive-Summary)\n",
    "- [Data Dictionary](#Data-Dictionary)\n",
    "- [Package Import](#Package-Import)\n",
    "- [Scraping COVID-19 Geo Tagged Tweet URLs](#Scraping-COVID-19-Geo-Tagged-Tweet-URLs)\n",
    "- [Hydrating Tweets using TWARC API](#Hydrating-Tweets-using-TWARC-API)\n",
    "- [Analyzing Twitter Data](#Analyzing-Twitter-Data)\n",
    "- [Visualization](#Visualization)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DVDzsSsj4-9O"
   },
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0CzJla59yCG"
   },
   "source": [
    "| Features       | Type    | Description                                        |\n",
    "|----------------|---------|----------------------------------------------------|\n",
    "| full_text      | object  | full text of the tweet                             |\n",
    "| id             | float64 | tweet id                                           |\n",
    "| date           | object  | date tweet was posted                              |\n",
    "| city           | object  | city from which tweet was posted                   |\n",
    "| country_code   | object  | country code from which related tweet was posted   |\n",
    "| country        | object  | country corresponding to country code              |\n",
    "| coordinates    | object  | latitude and longitudinal values of tweet location |\n",
    "| state          | object  | state from which tweet was posted                  |\n",
    "| classification | object  | overall tweet sentiment (positive or negative)     |\n",
    "| p_pos          | float64 | percent chance of positive sentiment               |\n",
    "| p_neg          | float64 | percent chance of negative sentiment               |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u20S3yVr4-0A"
   },
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTMT3Zkw7AB8"
   },
   "outputs": [],
   "source": [
    "# Standard Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling Packages\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "# Data Obtaining and Cleaning Packages\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rp5q6cmr6vAj"
   },
   "source": [
    "## Scraping COVID-19 Geo Tagged Tweet URLs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sourced our tweets from [this collection](https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset). It turned out that, once logged in, the page contains a list of direct links to the `.csv` files of tweet IDs for each day, so we downloaded the specific `<div>` containing this list and used `BeautifulSoup` and `requests` to download the files. Finally, we concatenate the full list and converted the `.csv` to a `.txt` out of python for easy access from `twarc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(open(\"../data/data_links.html\"), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csvs = []\n",
    "for i, link in list(enumerate(links)):\n",
    "    file = requests.get(link)\n",
    "    title = re.findall('(\\w+)(\\.\\w+)+(?!.*(\\w+)(\\.\\w+)+)', link)\n",
    "    title = ''.join(list(title[0]))\n",
    "    decoded_content = file.content.decode('utf-8')\n",
    "\n",
    "    cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "    my_list = list(cr)\n",
    "    all_csvs.append(pd.DataFrame(my_list))\n",
    "    pd.DataFrame(my_list).to_csv('../data/tweet_ids/' + title, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = pd.concat(all_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids.to_csv('../data/tweet_ids/all_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hydrating Tweets using TWARC API\n",
    "\n",
    "If you are following this process, ensure you have `pip install twarc`, then run the shell command `twarc hydrate ids.txt > tweets.jsonl` in the folder with the `ids.txt` file to download all the tweets in json form. This process took about 20 minutes for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBdQHSOIoIU-"
   },
   "source": [
    "## Analyzing Twitter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lqk7djVIluRD"
   },
   "outputs": [],
   "source": [
    "# Adapted from: https://medium.com/shiyan-boxer/2020-us-presidential-election-twitter-sentiment-analysis-and-visualization-89e58a652af5\n",
    "# Big thanks to Shiyan Boxer\n",
    "\n",
    "class TweetAnalyzer():\n",
    "    \"\"\"\n",
    "    Functionality for analyzing and categorizing content from tweets.\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_tweet(self, tweet):\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "    def analyze_sentiment(self, tweet):\n",
    "        return TextBlob(self.clean_tweet(tweet), analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "    def tweets_to_data_frame(self, tweets):\n",
    "        df = pd.DataFrame(data=[tweet['full_text']\n",
    "                                for tweet in tweets], columns=['full_text'])\n",
    "        df['id'] = np.array([tweet['id'] for tweet in tweets])\n",
    "        df['date'] = np.array([tweet['created_at'] for tweet in tweets])\n",
    "        df['city'] = [tweet['place']['full_name'] for tweet in tweets]\n",
    "        df['country_code'] = [tweet['place']['country_code']\n",
    "                              for tweet in tweets]\n",
    "        df['country'] = [tweet['place']['country'] for tweet in tweets]\n",
    "        df['coordinates'] = [tweet['coordinates']['coordinates']\n",
    "                             for tweet in tweets]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .jsonl of all tweets\n",
    "\n",
    "all_tweets = []\n",
    "with open('../data/tweets.jsonl', 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        all_tweets.append(result)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tweets which do not have proper geo fields\n",
    "all_tweets = [tweet for tweet in all_tweets if type(tweet['place']) == dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TweetAnalyzer class to convert our .jsonl to a pandas dataframe\n",
    "ta = TweetAnalyzer()\n",
    "df = ta.tweets_to_data_frame(all_tweets)\n",
    "\n",
    "# Keep only US data\n",
    "df = df[df['country_code'] == 'US']\n",
    "\n",
    "# Keep only tweets with convenient state labels\n",
    "df['state'] = [city[-2:] for city in df['city']]\n",
    "\n",
    "valid_states = ['OH', 'CA', 'MA', 'FL', 'IL', 'MD', 'NC', 'NY', 'AZ',\n",
    "                'LA', 'TX', 'UT', 'GA', 'NV', 'MI', 'NJ', 'IN', 'ME', 'KS', 'VA',\n",
    "                'MN', 'TN', 'PA', 'SC', 'WI', 'NM', 'OR', 'MO', 'WA', 'DC',\n",
    "                'AL', 'CT', 'ID', 'KY', 'MS', 'CO', 'OK', 'HI', 'AR', 'VT', 'RI',\n",
    "                'NH', 'MT', 'DE', 'NE',  'SD', 'IA', 'ND', 'WV',  'AK',\n",
    "                'WY']\n",
    "\n",
    "df = df[df['state'].isin(valid_states)]\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then needed to split our data into two halves to share the analysis time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:32316, :].to_csv(\n",
    "    '../data/tweets/cleaned_tweets_first_half.csv', index=False)\n",
    "df.loc[32317:, :].to_csv(\n",
    "    '../data/tweets/cleaned_tweets_second_half.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets/cleaned_tweets_second_half.csv')\n",
    "\n",
    "s = time.time()\n",
    "for i, tweet in enumerate(df['full_text']):\n",
    "    analysis = ta.analyze_sentiment(tweet).sentiment\n",
    "    df.loc[i, 'classification'] = analysis[0]\n",
    "    df.loc[i, 'p_pos'] = analysis[1]\n",
    "    df.loc[i, 'p_neg'] = analysis[2]\n",
    "    if i % 100 == 0:\n",
    "        print(\n",
    "            f'{i} of {df.shape[0]}, time elapsed: {(time.time() - s) / 60} minutes')\n",
    "        df.to_csv('../data/tweets/analyzed_tweets_second_half.csv', index=False)\n",
    "\n",
    "df.to_csv('../data/tweets/analyzed_tweets_second_half.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total runtime for one half of the data was `32200 of 32278, time elapsed: 2317.5798520445824 minutes\n",
    "`\n",
    "Total runtime for the other half was `32300 of 32317, time elapsed: 1938.4947881817818 minutes\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-merging files after we processed them on separate computers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('../data/tweets/analyzed_tweets_first_half.csv')\n",
    "df_2 = pd.read_csv('../data/tweets/analyzed_tweets_second_half.csv')\n",
    "df_3 = pd.concat([df_1, df_2])\n",
    "df_3.to_csv('../data/tweets/analyzed_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a slightly different version that uses `\\n` as seperator and `\\t` as text delimiter, which Tableau found easier to interpret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv('../data/tweets/analyzed_tweets_for_tableau.csv',\n",
    "            index=False, sep='\\n', quotechar='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We moved to Tableau to create the visualization, a timelapse of the four previous days rolling average sentiment by state. [You can view or download it yourself on Tableau Public.](https://public.tableau.com/views/COVIDTwitterSentimentVisualization/Sheet1?:language=en&:display_count=y&publish=yes&:origin=viz_share_link)\n",
    "\n",
    "![](../assets/coronavirus_sentiment_timelapse_w_legend.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VedZWcXL6GtP"
   },
   "source": [
    "# Conclusions and Recommendations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we analyzed the sentiments of COVID-19-related tweets in several ways. The overall trend shows that the public was initially optimistic, got much less cheerful in mid-April, then trended somewhat more positive throughout future months. We were also able to see the overall sentiment by state. You can especially track the major decrease in positive sentiment in NY and surrounding states after the pandemic reached its peak case load. Interestingly, there is not much of a visible decrease in sentiment over the course of the larger and more wide-spread second wave in our data.\n",
    "\n",
    "The fight against COVID-19 not only needs the guidance from the government but also a positive attitude from the public. Our analysis provides a potential approach to examine the public’s mood and allow institutions to respond to it in a timely manner.\n",
    "\n",
    "Our analysis has shown some relationships between geographic data and the general sentiments of the state during the pandemic. Moving forward, introducing more granular data such as the growth of confirmed cases or adding additional dimensionality to our sentiment analysis would help provide a more comprehensive picture. \n",
    "\n",
    "Our workflow was limited by the level of API access key granted by Twitter, preventing us from gathering much historical data on our own. We needed to find a pre-existing dataset, and would be well-served in the future to integrate additional COVID-related tweet datasets with what we were able to obtain.\n",
    "\n",
    "In addition, time constraints prevented us from testing a variety of pre-trained sentiment analysis models. The TextBlob Naive Bayes model was trained on IMDB movie review, making it somewhat less than ideal for our data, but most other well-known pre-trained models have a similar issue. Future work could use a more advanced sentiment analysis model, either by accessing a paid API or by hand-labeling tweets to create a custom-fitted model for this application.\n",
    "\n",
    "Finally, we would have liked to include a wider variety of visualizations of this data, included but not limited to annotations of major events, time series, and visualizations of more features such as retweets and likes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zszvWu3d6Gmx"
   },
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHnKjLgR4MP2"
   },
   "source": [
    "- COVID-19 Geo Tagged Tweets Dataset: https://ieee-dataport.org/open-access/coronavirus-covid-19-geo-tagged-tweets-dataset\n",
    "- Package for Hydrating Tweets: https://github.com/DocNow/twarc\n",
    "- TweetAnalyzer class adapted from: https://medium.com/shiyan-boxer/2020-us-presidential-election-twitter-sentiment-analysis-and-visualization-89e58a652af5\n",
    "- Everything You Need to Know About Sentiment Analysis: https://monkeylearn.com/sentiment-analysis/\n",
    "- Making a request to download csv: https://stackoverflow.com/questions/35371043/use-python-requests-to-download-csv\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "code",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
